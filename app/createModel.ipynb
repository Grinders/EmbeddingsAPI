{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') # use multilingual models for texts with non-english characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The pre-trained models produce embeddings of size 512 - 1024. However, when storing a large\n",
    "number of embeddings, this requires quite a lot of memory / storage.\n",
    "\n",
    "In this example, we reduce the dimensionality of the embeddings to e.g. 128 dimensions. This significantly\n",
    "reduces the required memory / storage while maintaining nearly the same performance.\n",
    "\n",
    "For dimensionality reduction, we compute embeddings for a large set of (representative) sentence. Then,\n",
    "we use PCA to find e.g. 128 principle components of our vector space. This allows us to maintain\n",
    "us much information as possible with only 128 dimensions.\n",
    "\n",
    "PCA gives us a matrix that down-projects vectors to 128 dimensions. We use this matrix\n",
    "and extend our original SentenceTransformer model with this linear downproject. Hence,\n",
    "the new SentenceTransformer model will produce directly embeddings with 128 dimensions\n",
    "without further changes needed.\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, evaluation, models, InputExample\n",
    "import logging\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#Model for which we apply dimensionality reduction\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#New size for the embeddings\n",
    "new_dimension = 32\n",
    "\n",
    "\n",
    "#We use AllNLI as a source of sentences to compute PCA\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "#We use the STS benchmark dataset to see how much performance we loose by using the dimensionality reduction\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "\n",
    "# We measure the performance of the original model\n",
    "# and later we will measure the performance with the reduces dimension size\n",
    "logger.info(\"Read STSbenchmark test dataset\")\n",
    "eval_examples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            eval_examples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "# Evaluate the original model on the STS benchmark dataset\n",
    "stsb_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name='sts-benchmark-test')\n",
    "\n",
    "logger.info(\"Original model performance:\")\n",
    "stsb_evaluator(model)\n",
    "\n",
    "######## Reduce the embedding dimensions ########\n",
    "\n",
    "#Read sentences from NLI dataset\n",
    "nli_sentences = set()\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        nli_sentences.add(row['sentence1'])\n",
    "        nli_sentences.add(row['sentence2'])\n",
    "\n",
    "nli_sentences = list(nli_sentences)\n",
    "random.shuffle(nli_sentences)\n",
    "\n",
    "#To determine the PCA matrix, we need some example sentence embeddings.\n",
    "#Here, we compute the embeddings for 20k random sentences from the AllNLI dataset\n",
    "pca_train_sentences = nli_sentences[0:20000]\n",
    "#pca_train_sentences.append(sentences)\n",
    "train_embeddings = model.encode(pca_train_sentences, convert_to_numpy=True)\n",
    "\n",
    "#Compute PCA on the train embeddings matrix\n",
    "pca = PCA(n_components=new_dimension)\n",
    "pca.fit(train_embeddings)\n",
    "pca_comp = np.asarray(pca.components_)\n",
    "\n",
    "# We add a dense layer to the model, so that it will produce directly embeddings with the new size\n",
    "dense = models.Dense(in_features=model.get_sentence_embedding_dimension(), out_features=new_dimension, bias=False, activation_function=torch.nn.Identity())\n",
    "dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))\n",
    "model.add_module('dense', dense)\n",
    "\n",
    "# Evaluate the model with the reduce embedding size\n",
    "logger.info(\"Model with {} dimensions:\".format(new_dimension))\n",
    "stsb_evaluator(model)\n",
    "\n",
    "\n",
    "# If you like, you can store the model on disc by uncommenting the following line\n",
    "model.save('models/MiniLM-'+str(new_dimension)+'dim-model')\n",
    "\n",
    "# You can then load the adapted model that produces new dimensional embeddings like this:\n",
    "#model = SentenceTransformer('models/MiniLM-'+str(new_dimension)+'dim-model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
